{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file gbi069_hw5.ipynb;\n",
    "# Open the file via Jupyter Notebook;\n",
    "# Run the code by clicking the \"Run\" button;\n",
    "# Everything should run correctly;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score as r2\n",
    "import pdb #for debugging the code. Set a breakpoint by pbd.set_trace(). https://www.geeksforgeeks.org/python-debugger-python-pdb/\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) y = x + 0.5 -> f(0.5) = 1, f(2) = 2.5, f(3) = 3.5\n",
      "    SSE = 0 + 0 + .25 = 0.25\n",
      "\n",
      "(b) y = x + 1 -> f(0.5) = 1.5, f(2) = 3, f(3) = 4\n",
      "    SSE = .25 + .25 + 1 = 1.5\n",
      "\n",
      "(c) y = 0.8*x + 0.3 -> f(0.5) = 0.7, f(2) = 1.9, f(3) = 2.7\n",
      "    SSE = .09 + .36 + .09 = 0.54\n",
      "\n",
      "(d) y = 0.8*x + 0.7 -> f(0.5) = 1.1, f(2) = 2.3, f(3) = 3.1\n",
      "    SSE = .01 + .04 + .01 = 0.06\n",
      "\n",
      "The best linear regression ussing SSE is (d)\n"
     ]
    }
   ],
   "source": [
    "##%% Question 1: Linear regression\n",
    "print(\"(a) y = x + 0.5 -> f(0.5) = 1, f(2) = 2.5, f(3) = 3.5\")\n",
    "print(\"    SSE = 0 + 0 + .25 = 0.25\")\n",
    "\n",
    "print(\"\\n(b) y = x + 1 -> f(0.5) = 1.5, f(2) = 3, f(3) = 4\")\n",
    "print(\"    SSE = .25 + .25 + 1 = 1.5\")\n",
    "\n",
    "print(\"\\n(c) y = 0.8*x + 0.3 -> f(0.5) = 0.7, f(2) = 1.9, f(3) = 2.7\")\n",
    "print(\"    SSE = .09 + .36 + .09 = 0.54\")\n",
    "\n",
    "print(\"\\n(d) y = 0.8*x + 0.7 -> f(0.5) = 1.1, f(2) = 2.3, f(3) = 3.1\")\n",
    "print(\"    SSE = .01 + .04 + .01 = 0.06\")\n",
    "\n",
    "print(\"\\nThe best linear regression ussing SSE is (d)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso and Ridge regression solve the issue of overfitting in linear regression.\n",
      "The main difference between the two regressions is that Lasso can exclude\n",
      "useless variables, while Ridge regression does better when most variables are useful.\n",
      "Ridge is advantageous because it reduces model complexity and multi-collinearity.\n",
      "Lasso is advantageous because it uses less predictors\n"
     ]
    }
   ],
   "source": [
    "##%% Question 2: Lasso and Ridge regression\n",
    "print(\"Lasso and Ridge regression solve the issue of overfitting in linear regression.\")\n",
    "print(\"The main difference between the two regressions is that Lasso can exclude\")\n",
    "print(\"useless variables, while Ridge regression does better when most variables are useful.\")\n",
    "print(\"Ridge is advantageous because it reduces model complexity and multi-collinearity.\")\n",
    "print(\"Lasso is advantageous because it uses less predictors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a)\n",
      "Node impurities\n",
      "Gini Index: a-left: 0.3750; right: 0.1049; b-left: 0.4444; right: 0.4444; c-left: 0.1723; right: 0.1975;\n",
      "------------------------------------------------\n",
      "Entropy: a-left: 0.8113; right: 0.3095; b-left: 0.9183; right: 0.9183; c-left: 0.4537; right: 0.5033;\n",
      "------------------------------------------------\n",
      "Misclass error: a-left: 0.2500; right: 0.0555; b-left: 0.3333; right: 0.3333; c-left: 0.0952; right: 0.1111;\n",
      "------------------------------------------------\n",
      "\n",
      "b)\n",
      "\n",
      "Qualities of splitting:\n",
      "Gini Index: a: 0.3074; b: 0.4444; c: 1799;\n",
      "------------------------------------------------\n",
      "Entropy: a: 0.5102; b: 0.9183; c: 0.4686;\n",
      "------------------------------------------------\n",
      "Misclass error: a: 0.1333; b: 0.3333; c: 0.1000;\n",
      "------------------------------------------------\n",
      "\n",
      "c)\n",
      "The best splitting for: \n",
      "Gini Index: c\n",
      "Entropy: c\n",
      "Misclass error: c\n",
      "\n",
      "d)\n",
      "All three methods do have the same best splitting, c\n"
     ]
    }
   ],
   "source": [
    "##%% Question 3: Decision tree\n",
    "print(\"a)\")\n",
    "print(\"Node impurities\")\n",
    "# 1 - x^2 - y^2\n",
    "print(\"Gini Index: a-left: 0.3750; right: 0.1049; b-left: 0.4444; right: 0.4444; c-left: 0.1723; right: 0.1975;\")\n",
    "print(\"------------------------------------------------\")\n",
    "# -xlog_2(x) - ylog_2(y)\n",
    "print(\"Entropy: a-left: 0.8113; right: 0.3095; b-left: 0.9183; right: 0.9183; c-left: 0.4537; right: 0.5033;\")\n",
    "print(\"------------------------------------------------\")\n",
    "# 1 - max(x, y)\n",
    "print(\"Misclass error: a-left: 0.2500; right: 0.0555; b-left: 0.3333; right: 0.3333; c-left: 0.0952; right: 0.1111;\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"\\nb)\")\n",
    "print(\"\\nQualities of splitting:\")\n",
    "print(\"Gini Index: a: 0.3074; b: 0.4444; c: 1799;\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Entropy: a: 0.5102; b: 0.9183; c: 0.4686;\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Misclass error: a: 0.1333; b: 0.3333; c: 0.1000;\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"\\nc)\")\n",
    "print(\"The best splitting for: \")\n",
    "print(\"Gini Index: c\")\n",
    "print(\"Entropy: c\")\n",
    "print(\"Misclass error: c\")\n",
    "\n",
    "print(\"\\nd)\")\n",
    "print(\"All three methods do have the same best splitting, c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Iris-versicolor ID => 0\n",
      "Class Iris-setosa ID => 1\n",
      "Class Iris-virginica ID => 2\n",
      "Scores: [96.66666666666667, 96.66666666666667, 100.0, 90.0, 100.0]\n",
      "Mean Accuracy: 96.667%\n"
     ]
    }
   ],
   "source": [
    "##%% Question 4: KNN\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "        print('Class %s ID => %d' % (value, i))\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\n",
    "    distance = 0.0\n",
    "    ### your code starts\n",
    "    for i in range(len(row1) - 1):\n",
    "        distance += pow(row1[i] - row2[i], 2)\n",
    "    ### your code ends\n",
    "    return sqrt(distance)\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors and return the list of neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "    ### your code starts\n",
    "        euclid = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, euclid))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    ### your code ends\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "      neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    ### your code starts\n",
    "    # get all neighbors and make the prediction based on majority of neighbors\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    class_pred = list()\n",
    "    for item in neighbors:\n",
    "        class_pred.append(item[-1])\n",
    "    prediction = mode(class_pred)\n",
    "    ### your code ends\n",
    "    return prediction\n",
    "\n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_classification(train, row, num_neighbors)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "\n",
    "\n",
    "# Test the kNN on the Iris Flowers dataset\n",
    "seed(1)\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "# the output is\n",
    "\n",
    "#The output scores is 96.66666666666667\n",
    "#The mean accuracy is 96.667%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error (Before Splitting) = 10/30\n",
      "Pessimistic Error = (10 + 0.5)/30 = 10.5/30\n",
      "Training Error (After Splitting) = 9/10\n",
      "Pessimistic Error (After Splitting) = (9 + 4*0.5)/30 = 11/30\n",
      "This tree should be pruned.\n"
     ]
    }
   ],
   "source": [
    "##%% Question 5: Pruning of decision tree\n",
    "print(\"Training Error (Before Splitting) = 10/30\")\n",
    "print(\"Pessimistic Error = (10 + 0.5)/30 = 10.5/30\")\n",
    "print(\"Training Error (After Splitting) = 9/10\")\n",
    "print(\"Pessimistic Error (After Splitting) = (9 + 4*0.5)/30 = 11/30\")\n",
    "print(\"This tree should be pruned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
